---
title: "Gill_Sarah_ML_PS4"
author: "Sarah Gill"
date: "2/28/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Documents/GitHub/problem-set-4")
library(ggplot2)
library(tidyverse)
library(skimr)

```

### Performing k-Means By Hand
```{r 1}
x <- cbind(c(1, 1, 0, 5, 6, 4), c(4, 3, 4, 1, 2, 0))

#1 Plot the observations
#plot(x)
df <- data.frame(x)

ggplot(df)+
  geom_point(aes(x=X1, y=X2))
```
2. Randomly assign a cluster label to each observation. Report the cluster labels for each observation and plot the results with a different color for each cluster.
```{r 2}
set.seed(246810)
cluster_lab <- as.factor(sample(c(0,1), size = nrow(x), replace = TRUE))

df$cluster_lab = cluster_lab

ggplot(df)+
  geom_point(aes(x=X1, y=X2, color = cluster_lab))

```
3.Compute the centroid for each cluster.
```{r 3}

centroid_0 <- c(mean(df$X1[df$cluster_lab==0]), mean(df$X2[df$cluster_lab==0]))
centroid_1 <- c(mean(df$X1[df$cluster_lab==1]), mean(df$X2[df$cluster_lab==1]))

ggplot(df)+
  geom_point(aes(x=X1, y=X2, color = cluster_lab))+
  geom_point(aes(x=centroid_0[1], y=centroid_0[2], color = cluster_lab), shape = 4) +
  geom_point(aes(x=centroid_1[1], y=centroid_1[2], color = cluster_lab[1]), shape = 4)
  
```
4. Assign each observation to the centroid to which it is closest, in terms of Euclidean distance. Report the cluster labels for each observation
```{r 4}
#Euclidean distance: d((x1,y1),(x2,y2) = sqrt((x1-x2)^2+(y1-y2)^2)

euclid_dist <- function (df, i, centroid_x){
  x1 <- df$X1[i]
  y1 <- df$X2[i]
  x2 <- centroid_x[1]
  y2 <- centroid_x[2]
  dist <- sqrt((x1-x2)^2+(y1-y2)^2)
  return(dist)
}



for (i in 1:6) {
if (euclid_dist(df, i ,centroid_0) < euclid_dist(df, i ,centroid_1)){
  df$cluster_lab[i] <- 0

} else {
  df$cluster_lab[i] <- 1
}
  }

ggplot(df)+
  geom_point(aes(x=X1, y=X2, color = cluster_lab))+
  geom_point(aes(x=centroid_0[1], y=centroid_0[2], color = cluster_lab), shape = 4) +
  geom_point(aes(x=centroid_1[1], y=centroid_1[2], color = cluster_lab[1]), shape = 4)
  

```

```{r 5a}
centroid_0 <- c(mean(df$X1[df$cluster_lab==0]), mean(df$X2[df$cluster_lab==0]))
centroid_1 <- c(mean(df$X1[df$cluster_lab==1]), mean(df$X2[df$cluster_lab==1]))

ggplot(df)+
  geom_point(aes(x=X1, y=X2, color = cluster_lab))+
  geom_point(aes(x=centroid_0[1], y=centroid_0[2], color = cluster_lab), shape = 4) +
  geom_point(aes(x=centroid_1[1], y=centroid_1[2], color = cluster_lab[1]), shape = 4)

```

```{r 5b}

for (i in 1:6) {
if (euclid_dist(df, i ,centroid_0) < euclid_dist(df, i ,centroid_1)){
  df$cluster_lab[i] <- 0

} else {
  df$cluster_lab[i] <- 1
}
  }

ggplot(df)+
  geom_point(aes(x=X1, y=X2, color = cluster_lab))+
  geom_point(aes(x=centroid_0[1], y=centroid_0[2], color = cluster_lab), shape = 4) +
  geom_point(aes(x=centroid_1[1], y=centroid_1[2], color = cluster_lab[1]), shape = 4)
 
```
No change, wiht the randomization that happened this time around, it only took one itteration before cluster assignement and location of centroids stopped changeing

6. Plot
See above

### Clustering State Legislative Professionalism

1. Load the state legislative professionalism data
```{r 2.1}
load("Data and Codebook/legprof-components.v1.0.RData")
legprof <- x
skim(legprof)

```
2. Munge the data
```{r 2.2}

lp_clean <- legprof %>%
  filter(sessid == "2009/10")  %>%
  select("expend", "salary_real", "t_slength", "slength") %>%
  na.omit() %>%
  scale()
  
#is.na(lp_clean) #check
#dist(lp_clean)
```
    e. and anything else you think necessary to get this subset of data into workable form (hint: consider storing the state names as a separate object to be used in plotting later) 

3. Diagnose clusterability in any way youâ€™d prefer (e.g., sparse sampling, ODI, etc.); display the results and discuss the likelihood that natural, non-random structure exist in these data. _Hint:_ We didn't cover how to do this R in class, but consider `dissplot()` from the `seriation` package, the `factoextra` package, and others for calculating, presenting, and exploring the clusterability of some feature space.   
```{r 2.3}

```



4. (5 points) Fit an **agglomerative hierarchical** clustering algorithm using any linkage method you prefer, to these data and present the results. Give a quick, high level summary of the output and general patterns. 



