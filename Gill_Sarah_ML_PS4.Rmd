---
title: "Gill_Sarah_ML_PS4"
author: "Sarah Gill"
date: "2/28/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Documents/GitHub/problem-set-4")
library(ggplot2)
library(tidyverse)
library(skimr)
library(seriation)
library(gridExtra)

```

### Performing k-Means By Hand
```{r 1}
x <- cbind(c(1, 1, 0, 5, 6, 4), c(4, 3, 4, 1, 2, 0))

#1 Plot the observations
#plot(x)
df <- data.frame(x)

ggplot(df)+
  geom_point(aes(x=X1, y=X2))
```
2. Randomly assign a cluster label to each observation. Report the cluster labels for each observation and plot the results with a different color for each cluster.
```{r 2}
set.seed(246810)
cluster_lab <- as.factor(sample(c(0,1), size = nrow(x), replace = TRUE))

df$cluster_lab = cluster_lab

ggplot(df)+
  geom_point(aes(x=X1, y=X2, color = cluster_lab))

```
3.Compute the centroid for each cluster.
```{r 3}

centroid_0 <- c(mean(df$X1[df$cluster_lab==0]), mean(df$X2[df$cluster_lab==0]))
centroid_1 <- c(mean(df$X1[df$cluster_lab==1]), mean(df$X2[df$cluster_lab==1]))

ggplot(df)+
  geom_point(aes(x=X1, y=X2, color = cluster_lab))+
  geom_point(aes(x=centroid_0[1], y=centroid_0[2], color = cluster_lab), shape = 4) +
  geom_point(aes(x=centroid_1[1], y=centroid_1[2], color = cluster_lab[1]), shape = 4)
  
```
4. Assign each observation to the centroid to which it is closest, in terms of Euclidean distance. Report the cluster labels for each observation
```{r 4}
#Euclidean distance: d((x1,y1),(x2,y2) = sqrt((x1-x2)^2+(y1-y2)^2)

euclid_dist <- function (df, i, centroid_x){
  x1 <- df$X1[i]
  y1 <- df$X2[i]
  x2 <- centroid_x[1]
  y2 <- centroid_x[2]
  dist <- sqrt((x1-x2)^2+(y1-y2)^2)
  return(dist)
}



for (i in 1:6) {
if (euclid_dist(df, i ,centroid_0) < euclid_dist(df, i ,centroid_1)){
  df$cluster_lab[i] <- 0

} else {
  df$cluster_lab[i] <- 1
}
  }

ggplot(df)+
  geom_point(aes(x=X1, y=X2, color = cluster_lab))+
  geom_point(aes(x=centroid_0[1], y=centroid_0[2], color = cluster_lab), shape = 4) +
  geom_point(aes(x=centroid_1[1], y=centroid_1[2], color = cluster_lab[1]), shape = 4)
  

```

```{r 5a}
centroid_0 <- c(mean(df$X1[df$cluster_lab==0]), mean(df$X2[df$cluster_lab==0]))
centroid_1 <- c(mean(df$X1[df$cluster_lab==1]), mean(df$X2[df$cluster_lab==1]))

ggplot(df)+
  geom_point(aes(x=X1, y=X2, color = cluster_lab))+
  geom_point(aes(x=centroid_0[1], y=centroid_0[2], color = cluster_lab), shape = 4) +
  geom_point(aes(x=centroid_1[1], y=centroid_1[2], color = cluster_lab[1]), shape = 4)

```

```{r 5b}

for (i in 1:6) {
if (euclid_dist(df, i ,centroid_0) < euclid_dist(df, i ,centroid_1)){
  df$cluster_lab[i] <- 0

} else {
  df$cluster_lab[i] <- 1
}
  }

ggplot(df)+
  geom_point(aes(x=X1, y=X2, color = cluster_lab))+
  geom_point(aes(x=centroid_0[1], y=centroid_0[2], color = cluster_lab), shape = 4) +
  geom_point(aes(x=centroid_1[1], y=centroid_1[2], color = cluster_lab[1]), shape = 4)
 
```
No change, wiht the randomization that happened this time around, it only took one itteration before cluster assignement and location of centroids stopped changeing

6. Plot
See above

### Clustering State Legislative Professionalism

1. Load the state legislative professionalism data
```{r 2.1}
load("Data and Codebook/legprof-components.v1.0.RData")
legprof <- x
#skim(legprof)

```
2. Munge the data
```{r 2.2}

lp_clean <- legprof %>%
  filter(sessid == "2009/10")  %>%
  select("expend", "salary_real", "t_slength", "slength") %>%
  na.omit() %>%
  scale()


state_name <- legprof %>%
  filter(sessid == "2009/10")  %>%
  na.omit() %>%
  select("fips", "stateabv", "state")

lp_for_reference <- legprof %>%
  filter(sessid == "2009/10")  %>%
  select("state", "stateabv", "expend", "salary_real", "t_slength", "slength") %>%
  na.omit()
 
#is.na(lp_clean) #check
#dist(lp_clean)
```
    e. and anything else you think necessary to get this subset of data into workable form (hint: consider storing the state names as a separate object to be used in plotting later) 

3. Diagnose clusterability  
```{r 2.3}
di <- dist(lp_clean)
dissplot(di)


```

This plot does not support the presence of non-random structure in this data. The central diagonal is very narrow and there are few rectangles of particularly dark pigmentation. The picture is generally grey overall, without much visually apparent structure. 


4. (5 points) Fit an **agglomerative hierarchical** clustering algorithm using any linkage method you prefer, to these data and present the results.
```{r 2.4}
hc_single <- hclust(di, 
                    method = "single");plot(hc_single, hang = -1)


hc_complete <- hclust(di, 
                      method = "complete"); plot(hc_complete, hang = -1) 

hc_average <- hclust(di, 
                     method = "average"); plot(hc_average, hang = -1)

hc_centroid <- hclust(di,
                      method = "centroid"); plot(hc_centroid, hang = -1)



```
At the k=2 level, all linkage methods except complete have one extremely small cluster and one large cluster (for instance at the point of thee being two clusters for linkage method = single, there is one state in a cluster and all other states in a cluster). This seems incorrect, however the test of clusterability did not support a high level of clusterability. 
Looking at linkage method = complete we see a lot of pair-wise clustering early on.

```{r 2.4b}
hc <- cutree(hc_complete, 
               k = 2)

hc_results <- data.frame(hc)
hc_results$state <- lp_for_reference$state

c1 <- hc_results%>%
  filter(hc_results$hc == 1)%>%
  select("state")

as.list(c1)
```
```{r 2.4c}
c2 <- hc_results%>%
  filter(hc_results$hc == 2)%>%
  select("state")

as.list(c2)
```
As mentioned above, if we use complete as our linkage method and assume that there are 2 clusters, then we see one large cluster and one small one (although still a reasonable size). 
The small cluster has mostly populous, high cost of living states, but not all of them have above average population density or cost of living. The larger cluster is too large to characterize without more information


5. (5 points) Fit a **k-means** algorithm to these data and present the results.
```{r 2.5a}
set.seed(246810)

kmeans <- kmeans(lp_clean[ ,2], 
                 centers = 2, #k 
                 nstart = 15) #start 15 times


kmeans_results <- data.frame(kmeans$cluster)
kmeans_results$state <- lp_for_reference$state

c1 <- kmeans_results%>%
  filter(kmeans$cluster == 1)%>%
  select("state")

as.list(c1)
```

```{r 2.5b}

c2 <- kmeans_results%>%
  filter(kmeans$cluster == 2)%>%
  select("state")

as.list(c2)

```
In one cluster we see many, highly populous, traditionally blue states that also have a high cost of liveing, such as California and New York. However, this cluster also has  Ohio which does not immediately appear related to the others, but has been blue and purple in the past.
In the other cluster we see some Southern states but also Southwestern and Eastern states too. Many are traditionally red states, but not all (for instance CT and CO are often blue states). Many of the states here have below average costs of living (but not all).
The second cluster also has more observations in it. 
I do not know much, a priori, about legislative professionalism, so cannot easily judge the results given by this kmeans clustering 

6. Fit a Gaussian mixture model via the EM algorithm*to these data and present the results. 

```{r 2.6}
library(mixtools) #journal of statistical software to accompony this
library(plotGMM)
set.seed(7355) # set seed for iterations to return to same place

gmm1 <- normalmixEM(lp_clean, k = 2) 
ggplot(data.frame(x = gmm1$x)) +
  geom_histogram(aes(x, ..density..), fill = "darkgray") +
  stat_function(geom = "line", fun = plot_mix_comps,
                args = list(gmm1$mu[1], gmm1$sigma[1], lam = gmm1$lambda[1]),
                colour = "darkred") +
  stat_function(geom = "line", fun = plot_mix_comps,
                args = list(gmm1$mu[2], gmm1$sigma[2], lam = gmm1$lambda[2]),
                colour = "darkblue") +
  xlab("Proffesionalism") +
  ylab("Density") + 
  theme_bw()

lp <- data.frame(lp_clean)
gmm_e <- normalmixEM(lp$expend, k = 2) # fit the GMM using EM and 2 cluters

ggplot(data.frame(x = gmm_e$x)) +
  geom_histogram(aes(x, ..density..), fill = "darkgray") +
  stat_function(geom = "line", fun = plot_mix_comps,
                args = list(gmm_e$mu[1], gmm_e$sigma[1], lam = gmm_e$lambda[1]),
                colour = "darkred") +
  stat_function(geom = "line", fun = plot_mix_comps,
                args = list(gmm_e$mu[2], gmm_e$sigma[2], lam = gmm_e$lambda[2]),
                colour = "darkblue") +
  xlab("Expendature") +
  ylab("Density") + 
  theme_bw()

gmm_s <- normalmixEM(lp$salary_real, k = 2) # fit the GMM using EM and 2 cluters

ggplot(data.frame(x = gmm_s$x)) +
  geom_histogram(aes(x, ..density..), fill = "darkgray") +
  stat_function(geom = "line", fun = plot_mix_comps,
                args = list(gmm_s$mu[1], gmm_s$sigma[1], lam = gmm_s$lambda[1]),
                colour = "darkred") +
  stat_function(geom = "line", fun = plot_mix_comps,
                args = list(gmm_s$mu[2], gmm_s$sigma[2], lam = gmm_s$lambda[2]),
                colour = "darkblue") +
  xlab("Salary") +
  ylab("Density") + 
  theme_bw()
```
```{r 2.6b}
posterior_e <- data.frame(cbind(gmm_e$x, gmm_e$posterior))
rownames(posterior_e) <- lp_for_reference$state
#round(head(posterior_e, 10), 3)

posterior_e$component <- ifelse(posterior_e$comp.1 > 0.3, 1, 2) #maually, say prob > 30% -> cluster 1
#table(posterior_e$component)

posterior_s <- data.frame(cbind(gmm_s$x, gmm_s$posterior))
rownames(posterior_s) <- lp_for_reference$state
#round(head(posterior_s, 10), 3)

posterior_s$component <- ifelse(posterior_s$comp.1 > 0.3, 1, 2) #maually, say prob > 30% -> cluster 1
table(posterior_s$component)
```


We see one high density cluster centered at negative normalized overal professionalism and a broad, low density cluster centered at below 2 normalized overal professionalism with tails expanding across the spread of the data. So we have one tight cluster and one broad one.

If we look specifically at different features, (for instance expenditures and salaries) we get different clustering. However, the structure of one dense and high frequency cluster and one broad and low frequency cluster are maintained



7. (15 points) Compare output of all in visually useful, simple ways
```{r 2.7}
#lp <- data.frame(lp_clean)
df <- merge(lp_for_reference, hc_results, on = state)
df <- merge(df, kmeans_results, on = state)
#df <- merge(df, gmm_results, on = state)

df$hc <- as.factor(df$hc)
df$kmeans.cluster <- as.factor(df$kmeans.cluster)



ggplot(data = df)+
  geom_point(aes(x = expend, y = salary_real, color = hc))

ggplot(data = df)+
  geom_point(aes(x = expend, y = salary_real, color = kmeans.cluster))

ggplot(data = df)+
  geom_point(aes(x = expend, y = t_slength, color = hc))

ggplot(data = df)+
  geom_point(aes(x = expend, y = t_slength, color = kmeans.cluster))

ggplot(data = df)+
  geom_point(aes(x = expend, y = slength, color = hc))

ggplot(data = df)+
  geom_point(aes(x = expend, y = slength, color = kmeans.cluster))



ggplot(data = df)+
  geom_point(aes(x = salary_real, y = t_slength, color = hc))

ggplot(data = df)+
  geom_point(aes(x = salary_real, y = t_slength, color = kmeans.cluster))

ggplot(data = df)+
  geom_point(aes(x = salary_real, y = slength, color = hc))

ggplot(data = df)+
  geom_point(aes(x = salary_real, y = slength, color = kmeans.cluster))




ggplot(data = df)+
  geom_point(aes(x = t_slength, y = slength, color = hc))

ggplot(data = df)+
  geom_point(aes(x = t_slength, y = slength, color = kmeans.cluster))



```


8. (5 points) Select a validation strategy 

```{r 2.8}
library(clValid)

cl <- clValid(lp_clean, nClust = 2:6, 
              clMethods = c("hierarchical", "kmeans"), 
              method = "complete",
              validation = c("stability", "internal"))
summary(cl)


```

9. (10 points) Discuss the validation output, e.g.,

  * What can you take away from the fit? 
   
  Hierarchical and kmeans have similar fits
  
  * Which approach is optimal? And optimal at what value of k? 
  
  Mesuring by silhoette width, kmeans is the best fit, at a k=2
  
  * What are reasons you could imagine selecting a technically “sub-optimal” clustering method, regardless of the validation statistics? 
  
  First of all, there are multiple, sometimes contradicting measures of fit, so the judgement call of which to use can influence the optimal choice. Beyond this questons of limitations on computing power and time could advantage some, less computatioally intensive, algorithms even if their fit is inferior
  
  

